# 爬虫

## 1.正则表达式

正则表达式就是定义一些特殊的符号来匹配不同的字符。

|    ^     | 匹配字符串开始的位置。                                       |
| :------: | ------------------------------------------------------------ |
|    $     | 匹配字符创结束的位置。                                       |
|    .     | 代表所有的单个字符，除了 \n \r                               |
| `[...]`  | 代表在 [] 范围内的字符，比如 [a-z] 就代表 a到z的字母         |
| `[^...]` | 跟 [...] 唱反调，代表不在 [] 范围内的字符                    |
|   {n}    | 匹配在 {n} 前面的东西，比如: o{2} 不能匹配 Bob 中的 o ，但是能匹配 food 中的两个o。 |

|  `*`   | 和 {0,} 一个样，匹配 * 前面的 0 次或多次。 比如 zo* 能匹配“z”、“zo”以及“zoo”。 |
| :----: | ------------------------------------------------------------ |
|  `+`   | 和{1，} 一个样，匹配 + 前面 1 次或多次。 比如 zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。 |
|  `？`  | 和{0,1} 一个样，匹配 ？前面 0 次或 1 次。                    |
|  a\|b  | 匹配 a 或者 b。                                              |
| `（）` | 匹配括号里面的内容。                                         |

是我们在匹配过程中最常使用到的，表示的就是匹配任意字符。.*？的 . 代表所有的单个字符，除了 \n \r

## 2.常用的包

### 2.1 request包

```python
res_html = requests.get(base_url, headers=headers, params=get_params(mid, chksm, exportkey, sn, pass_ticket,idx),
                            cookies=cookies)
```

url,headers,params,cookies:地址，请求头，请求参数，cookies

get和post方法。

### 2.2 BeautifulSoup包

```python
from bs4 import BeautifulSoup
```

find和findall方法常用

## 3.Scrapy框架

![image-20230414163833761](https://oss-img-fxk.oss-cn-beijing.aliyuncs.com/markdown/image-20230414163833761.png)

```bash
scrapy startproject spiderTaobao1 ---创建一个新的项目
scrapy genspider example example.com---创建爬虫
scrapy crawl douban -o douban.csv ---运行爬虫并保存到csv文件
scrapy crawl name ---运行name到爬虫
```

![](https://img-blog.csdnimg.cn/f67a52b97d8140b4a3b5f84c694133b2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6ZKi6ZOB55S35YS_,size_20,color_FFFFFF,t_70,g_se,x_16)





## 4.使用scrapy框架去爬取淘宝数据

### 4.1 使用selenium驱动谷歌浏览器

需要下载对应版本的驱动

```
bro = webdriver.Chrome(executable_path=WEBDRIVER_PATH)
option = Options()
option.add_experimental_option('excludeSwitches', ['enable-automation'])
option.add_experimental_option("detach", True)
script = 'Object.defineProperty(navigator, "webdriver", {get: () => false,});'
bro.execute_script(script)
```

### 4.2 模拟用户登录获取cookies

```
Demo20.py
```

- 使用selenium模拟人填入用户名，密码，点击登录按钮
- 淘宝存在滑块验证--验证一次（**重点在于滑块在一个frame中，需要转换页面，否则一直找不到那个元素**）

- 恶心的是淘宝滑块通过点击登录按钮之后，它还会让你在滑块验证一次。（再次调用SwitchFrame方法就可以了）
- 将登录之后获取的cookie放到taobao.json中

### 4.3 使用scrapy解析页面

#### 4.3.1 middlewares中使用下载中间件--解决爬的次数多了需要滑块验证

```
DownloaderMiddleware在下载前处理滑块问题
```

```
__init__中初始化浏览器，加入cookie信息
```

```
process_request如果有滑块验证模拟人工滑块解封，返回爬虫htmlresponse
```

```
pageSource = HtmlResponse(url=request.url, body=self.browser.page_source, encoding='utf-8')该方法用于返回网页检查中的源代码--用于捕捉动态加载的网页
```

主要用于解决用户多次访问需要验证和将网页动态内容捕捉给爬虫的问题

#### 4.3.2 taobao爬虫解析下载中间件返回的htmlresponse

```
keywords = read_keywords(KEY_WORDS_FILE)输入关键字的文件
```

格式如下

![image-20230421171611980](https://oss-img-fxk.oss-cn-beijing.aliyuncs.com/markdown/image-20230421171611980.png)



```
start_requests中进行url申请
```

```
parse(self, response)对网页源代码进行分析，这里有个问题就是淘宝有两套样式表（我目前爬到的），所以要分两种写（真的难受），用到css解析
```

![image-20230421193624685](https://oss-img-fxk.oss-cn-beijing.aliyuncs.com/markdown/image-20230421193624685.png)

#### 4.3.3 Itme中的字段--和数据库相对应

```
class TaobaoItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # img_src = scrapy.Field()
    title = scrapy.Field()#商品名
    prize = scrapy.Field()#商品价格
    people_buy = scrapy.Field()#商品购买人数
    shop_name = scrapy.Field()#店铺名称
    shop_url = scrapy.Field()#店铺url
    item_type = scrapy.Field()#商品种类
```

![image-20230421171019626](https://oss-img-fxk.oss-cn-beijing.aliyuncs.com/markdown/image-20230421171019626.png)

#### 4.3.4 popelines将itmes输出到数据库中

```
__init__连接到数据库
```

```
process_item爬到数据后调用数据库进行存储
```

```
close_spider关闭爬虫时，将剩余数据写入数据库，关闭数据库
```

#### 4.3.5 setting中配置一系列数据

![image-20230421193918148](https://oss-img-fxk.oss-cn-beijing.aliyuncs.com/markdown/image-20230421193918148.png)

keywords中保存了需要爬取的一些列关键字

![image-20230421193939900](https://oss-img-fxk.oss-cn-beijing.aliyuncs.com/markdown/image-20230421193939900.png)
